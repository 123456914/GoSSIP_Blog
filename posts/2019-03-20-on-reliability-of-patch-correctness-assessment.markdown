---
layout: post
title: "On Reliability of Patch Correctness Assessment"
date: 2019-03-20 11:59:17 +0800
comments: true
categories: 
---

作者：Xuan-Bach D. Le, Lingfeng Bao, David Lo, Xin Xia, Shanping Li, Corina Pasareanu

单位：Carnegie Mellon University, Zhejiang University City College, Singapore Management University, Monash University, Zhejiang University, NASA Ames Research Center

出处：ICSE '19

原文：<https://xin-xia.github.io/publication/icse192.pdf>

<hr/>
## Abstract

当前最先进的自动化软件修复技术（ASR：automatic software repair）均极度依赖于不完备的软件规格说明书或者测试集来生成补丁，这就有可能导致ASR工具生成的补丁是错误并且难以通用的。因此，研究人员通常会采用自动化标注（automated annotation）或者作者标注（author annotation）这两种方式来评估ASR工具生成补丁的正确性。然而，由于自动化标注难以确认补丁的实际正确性，作者标注又太过于主观，如何恰当的评估众多现有ASR技术的有效性已成为了一个亟待解决的难题。

本文评估了自动化标注与作者标注这两种补丁正确性评估方式的可靠性。作者首先随机选取了现有ASR工具生成的189个补丁，并招募了35个专业开发者作为独立标注者对这些补丁的正确性进行了评估，构造出一组正确性标签作为黄金组（gold set）。作者接着通过度量评分者间信度（inter-rater reliability）这一指标，证明了该黄金组是符合常规标准的高质量基准数据集。随后，作者将自动化标注与作者标注生成的标签与黄金组进行了比较，以评估这两种补丁正确性评估方式的可靠性。

<!--more-->
## 1 Introduction

在理想情况下，研究人员应使用完备的软件规格说明书以评估ASR工具生成补丁的正确性。然而在实际中，我们很难获取到完备的软件规格说明书。因此，ASR工具通常会将测试集作为评判补丁正确性的主要标准，即如果一个补丁通过了用于生成该补丁的测试集中的所有测试，则认为该补丁是正确的。但是，这种评估方式可能会造成所谓的补丁过拟合（patch overfitting），即存在有多个能够通过所有测试但依然不正确的补丁。

为了解决这个问题，研究人员采用了两种新的方式来评估ASR工具生成补丁的正确性：

+ **基于独立测试集的自动化标注（automated annotation by independent test suite）：**采用自动化测试用例生成工具（automatic test case generation tool）生成的独立测试集（ITS：independent test suite）评估ASR工具生成补丁的正确性。

+ **作者标注（author annotation）：**ASR工具的作者人工检查该工具与对照工具生成补丁的正确性。

上述两种补丁正确性评估方式均具有内在的缺陷，即自动化标注是不完备的，其难以确认补丁的实际正确性，而作者标注又太过于主观。

本文客观的评估了自动化标注与作者标注这两种补丁正确性评估方式的可靠性，并对于如何评估众多现有ASR技术的有效性这一难题提出了相应的见解。文章回答了三个研究问题：

+ **RQ1：**独立标注者能否对补丁的正确性达成共识？
+ **RQ2：**作者标注生成的补丁正确性标签的可靠性如何？
+ **RQ3：**自动化标注生成的补丁正确性标签的可靠性如何？

文章的贡献如下：

本文做出了如下贡献：

+ 本文是第一个研究自动化标注与作者标注这两种补丁正确性评估方式可靠性的文章。作者通过招募专业开发者进行用户研究，构造了一组正确性标签作为黄金组。通过与该黄金组进行比较，文章展示了现有ASR工具所采用的补丁正确性评估方式的优势和不足。
+ 基于本文的研究结果，作者对于如何评估众多现有ASR技术的有效性这一难题提出了相应的见解。尽管与作者标注相比，自动化标注的效果欠佳，ASR工具还是可以使用自动化标注辅助作者标注，提升评估补丁正确性的精度，同时降低人工评估补丁正确性的成本。

## 2 Background

文章在评估过程中涉及到的ASR工具包括：

+ ACS
+ Kali
+ GenProg
+ Nopol
+ S3
+ Angelix
+ JFix

## 3 User Study

文章招募了35个专业开发者作为独立标注者进行了用户研究，参与者需要通过判断ASR工具生成的补丁与实际的补丁在语义上是否等同来完成多项任务。

**补丁数据集**

文章将ACS、Nopol与S3发布的经过作者标注的补丁数据集作为评估对象。这三个补丁数据集同时标注了该工具（ACS、Nopol、S3）与对照工具（GenProg、Kali、Angelix、JFix）生成补丁的正确性。

这些补丁数据集均可以被用来作为自动化测试用例生成工具DIFFTGEN和RANDOOP的输入对象。

![20190317143250](/images/2019-03-20/20190317143250.png)

**任务设计**

在实验开始时，每个参与者都需要阅读一份用于简要阐述自动化程序修复以及他们需要完成任务的教程。随后，他们便可以通过Web界面逐个完成任务。Web界面将会记录参与者的答案以及参与者完成每项任务的用时。

![20190317143356](/images/2019-03-20/20190317143356.png)

**参与者和任务分配**

文章从工业界招募了35个专业开发者进行了用户研究，其中有33人供职于两家大型软件开发公司，而其余两人则是一家教育机构的工程师。全部35个开发者都使用Java作为主要的编程语言，他们的平均工作年限为3.5年。作者将那些工作年限低于3年的开发者视为初级开发者，共20人，将那些工作年限高于3年的开发者视为高级开发者，共15人。

文章将35个专业开发者分为7组，每组中初级开发者与高级开发者的比例大致相同，ASR工具生成的每个补丁都经由同组的5个开发者重复标记。

## 4 Assessing Independent Annotators' Labels

在使用上节中用户研究生成的补丁正确性标签之前，文章需要首先评估这些标签的质量。本节回答了研究问题RQ1，即独立标注者能否对补丁的正确性达成共识？

![20190317153816](/images/2019-03-20/20190317153816.png)

All Agree - Unk：忽略所有结果为unknown的标签。

通过计算，用户研究生成的补丁正确性标签的平均配对Cohen's kappa系数与Krippendorff's alpha系数分别为0.691与0.734。这些数据表明，参与者之间达成了实质性的一致，证明了这些标签是符合常规标准的高质量基准数据集。

文章进一步进行了两项合理性检查，以证明独立标注者的决定并不是随意的。首先，作者认为负责任的标注者会花费更多的时间来检查那些最终被标记为unknown的补丁。文章通过绘制箱型图（box plot）的方式比较了标注者检查那些被标记为confirmed（correct或incorrect）的补丁与那些被标记为unknown的补丁所花费时间的长短。

![20190317153842](/images/2019-03-20/20190317153842.png)

其次，作者认为负责任的标注者会花费更多的时间来检查困难的补丁而不是简单的补丁。同时，作者认为标注者之间的分歧可以作为衡量补丁难度的一项指标。文章比较了标注者检查那些意见完全一致的补丁与那些存在意见分歧的补丁所花费时间的长短。

![20190317153919](/images/2019-03-20/20190317153919.png)

## 5 Assessing Author Annotation

文章在只考虑All Agree与Majority Agree这两个版本的补丁数据集的前提下，将作者标注生成与用户研究生成的标签进行了比较。本节回答了研究问题RQ2，即作者标注生成的补丁正确性标签的可靠性如何？

![20190317171820](/images/2019-03-20/20190317171820.png)

通过计算，作者标注生成与用户研究生成的标签在All Agree与Majority Agree这两个版本的补丁数据集下的Cohen's kappa系数分别为0.719与0.697，Krippendorff's alpha系数分别为0.717与0.695。这些数据表明，大多数（88.8-89.0%）作者标注生成与用户研究生成的标签是匹配的。评分者间信度这一指标表明作者标注生成与用户研究生成的标签之间达成了实质性的一致。

![20190318001542](/images/2019-03-20/20190318001542.png)

文章比较了标注者检查那些与作者标注生成的标签一致的补丁和那些与作者标注生成的标签不一致的补丁所花费时间的长短。同时，作者认为标注者检查补丁花费时间的长短可以作为衡量补丁难度的一项指标。结果表明，作者标注生成与用户研究生成的标签之间的分歧往往出现在困难的补丁中。

![20190317171938](/images/2019-03-20/20190317171938.png)

## 6 Assessing Automated Annotation

文章将自动化标注生成与用户研究生成的标签进行了比较。本节回答了研究问题RQ3，即自动化标注生成的补丁正确性标签的可靠性如何？

文章采用了两个已有的自动化测试用例生成工具DIFFTGEN和RANDOOP来生成独立测试集（ITS），自动化标注All Agree与Majority Agree这两个版本的补丁数据集，并将自动化标注生成与用户研究生成的标签进行了比较。

![20190317171938](/images/2019-03-20/20190317235131.png)

由自动化测试用例生成工具DIFFTGEN和RANDOOP生成的独立测试集只能在两个版本的补丁数据集中标记不到五分之一的错误补丁。但是，自动化标注还是可以作为作者标注的辅助，以提升评估补丁正确性的精度，同时降低人工评估补丁正确性的成本。

![20190317235948](/images/2019-03-20/20190317235948.png)

## 7 Discussion

本文针对如何评估众多现有ASR技术的有效性这一难题提出了以下见解：

+ ASR工具的作者应该向社区公开其评估补丁正确性的补丁数据集；
+ 社区需要协同合作来分散昂贵的ASR技术评估成本；
+ ASR工具不应该单独采用自动化标注的方式来评估ASR技术的有效性；
+ 尽管自动化标注的效果欠佳，ASR工具还是可以使用自动化标注辅助作者标注。

![20190318002843](/images/2019-03-20/20190318002843.png)
